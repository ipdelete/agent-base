# ==============================================================================
# Agent Settings
# ==============================================================================
AGENT_DATA_DIR=~/.agent
AGENT_LOG_LEVEL=info

# ==============================================================================
# LLM Provider Configuration
# ==============================================================================
# Supported providers: local, openai, anthropic, azure, foundry, gemini
# Recommended: local (free, runs with Docker Desktop)
# Optional: Override the default model for any provider
LLM_PROVIDER=local
# AGENT_MODEL=phi4


# ==============================================================================
# Local Provider Configuration (when LLM_PROVIDER=local) - RECOMMENDED
# ==============================================================================
# Docker Desktop Model Runner with OpenAI-compatible API
# Requires: Docker Desktop with Model Runner enabled
# Setup:
#   1. docker desktop enable model-runner --tcp=12434
#   2. docker model pull phi4
# Note: Use full model ID from 'docker model list' or Docker Desktop UI
# Recommended: phi4 for speed, ai/qwen3 for best tool calling accuracy
LOCAL_BASE_URL=http://localhost:12434/engines/llama.cpp/v1
AGENT_MODEL=phi4

# ==============================================================================
# OpenAI Configuration (when LLM_PROVIDER=openai)
# ==============================================================================
# OPENAI_API_KEY=your-api-key-here

# ==============================================================================
# Anthropic Configuration (when LLM_PROVIDER=anthropic)
# ==============================================================================
# ANTHROPIC_API_KEY=your-api-key-here

# ==============================================================================
# Azure OpenAI Configuration (when LLM_PROVIDER=azure)
# ==============================================================================
# Requires: az login OR AZURE_OPENAI_API_KEY
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_DEPLOYMENT_NAME=your-deployment-name
# AZURE_OPENAI_VERSION=2024-08-01-preview
# AZURE_OPENAI_API_KEY=your-api-key-here

# ==============================================================================
# Azure AI Foundry Configuration (when LLM_PROVIDER=azure_ai_foundry)
# ==============================================================================
# Requires: az login (uses AzureCliCredential)
# AZURE_PROJECT_ENDPOINT=https://your-project.services.ai.azure.com/api/projects/your-project-id
# AZURE_MODEL_DEPLOYMENT=your-deployment-name

# ==============================================================================
# Google Gemini Configuration (when LLM_PROVIDER=gemini)
# ==============================================================================
# Option 1: Gemini Developer API (API key authentication)
# Get your API key from: https://aistudio.google.com/apikey
# GEMINI_API_KEY=your-api-key-here
# AGENT_MODEL=gemini-2.0-flash-exp

# Option 2: Vertex AI (GCP authentication with default credentials)
# Requires: gcloud auth application-default login
# GEMINI_USE_VERTEXAI=true
# GEMINI_PROJECT_ID=your-gcp-project-id
# GEMINI_LOCATION=us-central1
# AGENT_MODEL=gemini-2.5-pro

# ==============================================================================
# System Prompt Configuration
# ==============================================================================
# System prompt loading priority:
# 1. AGENT_SYSTEM_PROMPT (explicit override) - use this variable
# 2. ~/.agent/system.md (user's default) - create this file to customize globally
# 3. Agent default (src/agent/prompts/system.md) - fallback if above don't exist
#
# AGENT_SYSTEM_PROMPT=/path/to/prompt.md


# ==============================================================================
# Observability Configuration (OpenTelemetry)
# ==============================================================================
# Enable OpenTelemetry instrumentation for traces, logs, and metrics
# ENABLE_OTEL=true
# OTLP_ENDPOINT=http://localhost:4317

# Azure Application Insights connection string
# APPLICATIONINSIGHTS_CONNECTION_STRING=InstrumentationKey=xxx;IngestionEndpoint=https://xxx.in.applicationinsights.azure.com/