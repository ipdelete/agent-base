# ==============================================================================
# LLM Provider Configuration
# ==============================================================================
# Supported providers: openai, anthropic, azure, azure_ai_foundry
LLM_PROVIDER=openai

# ==============================================================================
# OpenAI Configuration (when LLM_PROVIDER=openai)
# ==============================================================================
OPENAI_API_KEY=your-api-key-here
OPENAI_MODEL=gpt-5-mini

# ==============================================================================
# Anthropic Configuration (when LLM_PROVIDER=anthropic)
# ==============================================================================
# ANTHROPIC_API_KEY=your-api-key-here
# ANTHROPIC_MODEL=claude-sonnet-4-5-20250929

# ==============================================================================
# Azure OpenAI Configuration (when LLM_PROVIDER=azure)
# ==============================================================================
# Requires: az login OR AZURE_OPENAI_API_KEY
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_DEPLOYMENT_NAME=gpt-5-codex
# AZURE_OPENAI_VERSION=2024-08-01-preview
# AZURE_OPENAI_API_KEY=your-api-key-here

# ==============================================================================
# Azure AI Foundry Configuration (when LLM_PROVIDER=azure_ai_foundry)
# ==============================================================================
# Requires: az login (uses AzureCliCredential)
# AZURE_PROJECT_ENDPOINT=https://your-project.services.ai.azure.com/api/projects/your-project-id
# AZURE_MODEL_DEPLOYMENT=gpt-4o

# ==============================================================================
# Agent Settings
# ==============================================================================
AGENT_DATA_DIR=~/.agent
LOG_LEVEL=info

# ==============================================================================
# System Prompt Configuration
# ==============================================================================
# Optional: Custom system prompt file (supports absolute and relative paths)
# If not set, uses default prompt from src/agent/prompts/system.md
# Supports placeholders: {{DATA_DIR}}, {{MODEL}}, {{SESSION_DIR}}, {{PROVIDER}}, {{MEMORY_ENABLED}}
# AGENT_SYSTEM_PROMPT=~/my-custom-prompt.md
# AGENT_SYSTEM_PROMPT=/path/to/prompt.md
# AGENT_SYSTEM_PROMPT=./prompts/custom.md

# ==============================================================================
# Memory Configuration
# ==============================================================================
# Enable in-memory storage for conversation context
MEMORY_ENABLED=true
# Memory type: in_memory (default), mem0, langchain (future)
MEMORY_TYPE=in_memory
# Memory storage directory (default: ~/.agent/memory)
# MEMORY_DIR=~/.agent/memory
# Max memories to inject as context (default: 20)
# MEMORY_HISTORY_LIMIT=20

# ==============================================================================
# Observability Configuration (OpenTelemetry)
# ==============================================================================
# Enable OpenTelemetry instrumentation for traces, logs, and metrics
# ENABLE_OTEL=true

# WARNING: Only enable in development! Captures prompts and LLM responses
# This can expose sensitive data and should NEVER be enabled in production
# ENABLE_SENSITIVE_DATA=false

# Azure Application Insights connection string
# Get from Azure Portal: Application Insights > Overview > Connection String
# APPLICATIONINSIGHTS_CONNECTION_STRING=InstrumentationKey=xxx;IngestionEndpoint=https://xxx.in.applicationinsights.azure.com/

# OTLP endpoint for traces and metrics (e.g., Aspire Dashboard, Jaeger, Prometheus)
# Default: http://localhost:4317 (works with local telemetry dashboard)
# OTLP_ENDPOINT=http://localhost:4317

# VS Code AI Toolkit extension port (for local development)
# VS_CODE_EXTENSION_PORT=8080

# Example configurations:
# ----------------------
# 1. Local development with Aspire Dashboard:
#    docker run --rm -it -p 18888:18888 -p 4317:18889 mcr.microsoft.com/dotnet/aspire-dashboard:latest
#    ENABLE_OTEL=true
#    OTLP_ENDPOINT=http://localhost:4317
#    Dashboard: http://localhost:18888
#
# 2. Azure Application Insights (production):
#    ENABLE_OTEL=true
#    APPLICATIONINSIGHTS_CONNECTION_STRING=InstrumentationKey=xxx;IngestionEndpoint=https://xxx
#
# 3. Multiple exporters (Aspire + App Insights):
#    ENABLE_OTEL=true
#    OTLP_ENDPOINT=http://localhost:4317
#    APPLICATIONINSIGHTS_CONNECTION_STRING=InstrumentationKey=xxx

# Documentation:
# https://opentelemetry.io/docs/concepts/signals/
# https://learn.microsoft.com/en-us/azure/azure-monitor/app/opentelemetry-enable
