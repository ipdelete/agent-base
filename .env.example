# ==============================================================================
# Agent Settings
# ==============================================================================
AGENT_DATA_DIR=~/.agent
AGENT_LOG_LEVEL=info

# ==============================================================================
# LLM Provider Configuration
# ==============================================================================
# Supported providers: openai, anthropic, azure, foundry, gemini, local
# Optional: Override the default model for any provider
LLM_PROVIDER=openai
# AGENT_MODEL=gpt-5-mini


# ==============================================================================
# OpenAI Configuration (when LLM_PROVIDER=openai)
# ==============================================================================
OPENAI_API_KEY=your-api-key-here

# ==============================================================================
# Anthropic Configuration (when LLM_PROVIDER=anthropic)
# ==============================================================================
# ANTHROPIC_API_KEY=your-api-key-here

# ==============================================================================
# Azure OpenAI Configuration (when LLM_PROVIDER=azure)
# ==============================================================================
# Requires: az login OR AZURE_OPENAI_API_KEY
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_DEPLOYMENT_NAME=your-deployment-name
# AZURE_OPENAI_VERSION=2024-08-01-preview
# AZURE_OPENAI_API_KEY=your-api-key-here

# ==============================================================================
# Azure AI Foundry Configuration (when LLM_PROVIDER=azure_ai_foundry)
# ==============================================================================
# Requires: az login (uses AzureCliCredential)
# AZURE_PROJECT_ENDPOINT=https://your-project.services.ai.azure.com/api/projects/your-project-id
# AZURE_MODEL_DEPLOYMENT=your-deployment-name

# ==============================================================================
# Google Gemini Configuration (when LLM_PROVIDER=gemini)
# ==============================================================================
# Option 1: Gemini Developer API (API key authentication)
# Get your API key from: https://aistudio.google.com/apikey
# GEMINI_API_KEY=your-api-key-here
# AGENT_MODEL=gemini-2.0-flash-exp

# Option 2: Vertex AI (GCP authentication with default credentials)
# Requires: gcloud auth application-default login
# GEMINI_USE_VERTEXAI=true
# GEMINI_PROJECT_ID=your-gcp-project-id
# GEMINI_LOCATION=us-central1
# AGENT_MODEL=gemini-2.5-pro

# ==============================================================================
# Local Provider Configuration (when LLM_PROVIDER=local)
# ==============================================================================
# Docker Desktop Model Runner with OpenAI-compatible API
# Requires: Docker Desktop with Model Runner enabled
# Enable: docker desktop enable model-runner --tcp=12434
# Pull model: docker model pull <model-name>
# Note: Use full model ID from 'curl http://localhost:12434/engines/llama.cpp/v1/models'
# Recommended: ai/qwen3 for best tool calling accuracy
# LOCAL_BASE_URL=http://localhost:12434/engines/llama.cpp/v1
# AGENT_MODEL=ai/qwen3
# AGENT_MODEL=ai/phi4


# ==============================================================================
# System Prompt Configuration
# ==============================================================================
# System prompt loading priority:
# 1. AGENT_SYSTEM_PROMPT (explicit override) - use this variable
# 2. ~/.agent/system.md (user's default) - create this file to customize globally
# 3. Agent default (src/agent/prompts/system.md) - fallback if above don't exist
#
# AGENT_SYSTEM_PROMPT=/path/to/prompt.md


# ==============================================================================
# Observability Configuration (OpenTelemetry)
# ==============================================================================
# Enable OpenTelemetry instrumentation for traces, logs, and metrics
# ENABLE_OTEL=true
# OTLP_ENDPOINT=http://localhost:4317

# Azure Application Insights connection string
# APPLICATIONINSIGHTS_CONNECTION_STRING=InstrumentationKey=xxx;IngestionEndpoint=https://xxx.in.applicationinsights.azure.com/