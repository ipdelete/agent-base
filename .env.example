# ==============================================================================
# Agent Settings
# ==============================================================================
AGENT_DATA_DIR=~/.agent
AGENT_LOG_LEVEL=info

# ==============================================================================
# LLM Provider Configuration
# ==============================================================================
# Supported providers: local, openai, anthropic, azure, foundry, gemini
# Recommended: local (free, runs with Docker Desktop)
# Optional: Override the default model for any provider
LLM_PROVIDER=local


# ==============================================================================
# Local Provider Configuration (when LLM_PROVIDER=local)
# ==============================================================================
# Docker Desktop Model Runner with OpenAI-compatible API
# Requires: Docker Desktop with Model Runner enabled
# Setup:
#   1. docker desktop enable model-runner --tcp=12434
#   2. docker model pull phi4
# Note: Use full model ID from 'docker model list' or Docker Desktop UI
# Recommended: phi4 for speed, or qwen3 for tool calling accuracy
LOCAL_BASE_URL=http://localhost:12434/engines/llama.cpp/v1
AGENT_MODEL=ai/phi4

# ==============================================================================
# OpenAI Configuration (when LLM_PROVIDER=openai)
# ==============================================================================
# OPENAI_API_KEY=your-api-key-here

# ==============================================================================
# Anthropic Configuration (when LLM_PROVIDER=anthropic)
# ==============================================================================
# ANTHROPIC_API_KEY=your-api-key-here

# ==============================================================================
# Google Gemini Configuration (when LLM_PROVIDER=gemini)
# ==============================================================================
# Gemini Developer API (API key authentication)
# Get your API key from: https://aistudio.google.com/apikey
# GEMINI_API_KEY=your-api-key-here

# ==============================================================================
# Azure OpenAI Configuration (when LLM_PROVIDER=azure)
# ==============================================================================
# Requires: az login OR AZURE_OPENAI_API_KEY
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
# AZURE_OPENAI_DEPLOYMENT_NAME=your-deployment-name
# AZURE_OPENAI_VERSION=2025-03-01-preview
# AZURE_OPENAI_API_KEY=your-api-key-here

# ==============================================================================
# Azure AI Foundry Configuration (when LLM_PROVIDER=foundry)
# ==============================================================================
# Requires: az login (uses AzureCliCredential)
# AZURE_PROJECT_ENDPOINT=https://your-project.services.ai.azure.com/api/projects/your-project-id

# ==============================================================================
# System Prompt Configuration
# ==============================================================================
# System prompt loading priority:
# 1. AGENT_SYSTEM_PROMPT (explicit override) - use this variable
# 2. ~/.agent/system.md (user's default) - create this file to customize globally
# 3. Agent default (src/agent/prompts/system.md) - fallback if above don't exist
#
# AGENT_SYSTEM_PROMPT=/path/to/prompt.md

# ==============================================================================
# Observability Configuration (OpenTelemetry)
# ==============================================================================
# Auto-detection: When ENABLE_OTEL is NOT set, telemetry automatically enables
# when the endpoint is reachable (e.g., after running 'agent --telemetry start')
#
# ENABLE_OTEL=false  # Force disable (even if endpoint is running)
# OTLP_ENDPOINT=http://localhost:4317

# ==============================================================================
# Observability Configuration (Foundry)
# ==============================================================================
# When Application Insights is connected to an Azure AI Foundry project.
# APPLICATIONINSIGHTS_CONNECTION_STRING=InstrumentationKey=xxx;IngestionEndpoint=https://xxx.in.applicationinsights.azure.com/

# ==============================================================================
# Memory Configuration
# ==============================================================================
# Memory type: "in_memory" (default, keyword search) or "mem0" (semantic search)
# MEMORY_TYPE=in_memory

# ------------------------------------------------------------------------------
# Mem0 Semantic Memory (when MEMORY_TYPE=mem0)
# ------------------------------------------------------------------------------
# Requirements: LLM_PROVIDER must be: openai, anthropic, azure, or gemini
# (local and foundry providers are not supported by mem0)
#
# Default: Local mode with Chroma vector database (zero configuration needed)
# Storage: ~/.agent/memory/chroma_db (SQLite-based)
# Namespace: $USER (automatic)

# Optional: Custom storage location
# MEM0_STORAGE_PATH=/custom/path/chroma_db

# Optional: Override user namespace (defaults to $USER)
# MEM0_USER_ID=your-username

# Optional: Project namespace for multi-project isolation
# MEM0_PROJECT_ID=my-project

# Optional: Max memories to inject as context (default: 20)
# MEMORY_HISTORY_LIMIT=20

# ------------------------------------------------------------------------------
# Advanced: Cloud Mode (mem0.ai managed service)
# ------------------------------------------------------------------------------
# Requires: mem0.ai account (https://app.mem0.ai)
# Note: Both API key AND org ID required to activate cloud mode
# MEM0_API_KEY=m0-your-api-key-here
# MEM0_ORG_ID=your-org-id-here